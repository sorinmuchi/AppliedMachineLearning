{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time, random, re, pprint, string\n",
    "import sys\n",
    "import pandas as pd \n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from itertools import islice, chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_input = pd.read_csv('train_input.csv')\n",
    "train_output = pd.read_csv('train_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delchars = ''.join(c for c in map(chr, range(256)) if not c.isalpha())\n",
    "delchars = ''.join(ch for ch in delchars if ch !=' ')\n",
    "\n",
    "def textClean(text):\n",
    "    #cleaning the html\n",
    "    output = re.sub('\\<.*?\\>','', text)\n",
    "    output = re.sub('\\@.*?\\s','', output)\n",
    "    output = re.sub('\\n','', output)\n",
    "    output = output.translate(str.maketrans('','',delchars))\n",
    "    return output\n",
    "\n",
    "def removeStopwords(word_list): \n",
    "    return list(set(word_list) - set(stopwords.words('English')))\n",
    "\n",
    "count_stops = Counter(stopwords.words(\"English\")*100)\n",
    "\n",
    "def removeStopwordsCount(words_list):\n",
    "    return Counter(words_list) - count_stops\n",
    "\n",
    "def findNGrams(input_list, n):\n",
    "    grams = list(zip(*[input_list[i:] for i in range(n)]))\n",
    "    return [''.join(x) for x in grams]\n",
    "\n",
    "stopword_set = set(stopwords.words(\"English\") + list(string.ascii_lowercase))\n",
    "#Stopwords + Individual letters\n",
    "\n",
    "def removeStopwordsList(word_list): \n",
    "    return [word for word in word_list if word not in stopword_set]\n",
    "\n",
    "def generate_features(dataframe):\n",
    "    start = time.time()\n",
    "    dataframe[\"text\"] = dataframe[\"conversation\"].apply(lambda x: textClean(x))\n",
    "    dataframe[\"words\"] = dataframe[\"text\"].apply(lambda x: x.split()) \n",
    "    dataframe[\"words\"] = dataframe[\"words\"].apply(lambda x: removeStopwordsList(x))\n",
    "    dataframe[\"words\"] = dataframe[\"words\"].apply(lambda x: x + findNGrams(x,2) + findNGrams(x,3))\n",
    "    dataframe[\"words_count\"] = dataframe[\"words\"].apply(lambda x: Counter(x))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex done! : 3.5700278282165527\n",
      "Stopwords done! : 96.16917991638184\n",
      "ngrams done! : 188.8405728340149\n"
     ]
    }
   ],
   "source": [
    "#stripping out all stopwords\n",
    "train_input = generate_features(train_input)\n",
    "train_input[\"output\"] = train_output[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeSum(list):\n",
    "    if len(list) < 2:\n",
    "        return list.iloc[0]\n",
    "    else: \n",
    "        mid = len(list)//2\n",
    "        \n",
    "        right = mergeSum(list[mid:])\n",
    "        left = mergeSum(list[:mid])\n",
    "\n",
    "        sum = left + right\n",
    "        return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_counter = Counter({k: v for k, v in total_counter.items() if v > 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Word Counter for each group\n",
    "groups_counter = train_input.groupby(\"output\")[\"words\"].apply(lambda x: mergeSum(x))\n",
    "groups_counter = groups_counter.apply(lambda x: Counter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_wordcount = sum(total_counter.values())\n",
    "total_word_freq = Counter({k:v/total_wordcount for k,v in total_counter.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_word_freq = {}\n",
    "groups_freq = groups_counter\n",
    "for label in train_input[\"output\"].unique():\n",
    "    group_wordcount = sum(groups_counter[label].values())\n",
    "    group_word_freq[label] = Counter({k:v/group_wordcount for k,v in groups_freq[label].items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetValDict(my_dict,key):\n",
    "    if key in my_dict: \n",
    "        return my_dict[key]\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "group_word_laplace = {}\n",
    "total_words = len(total_counter)\n",
    "\n",
    "for label in train_input[\"output\"].unique():\n",
    "    temp_group = groups_counter[label]\n",
    "    group_wordcount = sum(groups_counter[label].values())\n",
    "    #Conditional probability calculation with laplace smoothing\n",
    "    group_word_laplace[label] = Counter({k:(GetValDict(temp_group,k)+1)/(v + total_words)for k,v in total_counter.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build a counter for the IDFs of each word in our corpus\n",
    "words_doc_list = [list(counter.keys()) for counter in list(train_input[\"words_count\"].values)]\n",
    "words_doc_counter = Counter(chain.from_iterable(set(x) for x in words_doc_list))\n",
    "num_docs = len(train_input)\n",
    "total_words_idf = Counter({k: np.log(num_docs / words_doc_counter[k]) for k in total_counter.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_word_idf = {}\n",
    "total_words = len(total_counter)\n",
    "for label in train_input[\"output\"].unique():\n",
    "    temp_group = groups_counter[label]\n",
    "    group_wordcount = sum(groups_counter[label].values())\n",
    "    #Conditional probability calculation with laplace smoothing\n",
    "    group_word_idf[label] = Counter({k:((temp_group[k] + 1) / (v + total_words) * max(total_words_idf[k],0.3) )for k,v in total_counter.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now, create a function to predict class for each text snippet \n",
    "categories = train_input[\"output\"].unique()\n",
    "class_priors = {}\n",
    "for category in categories:\n",
    "    class_priors[category] = train_input.groupby(\"output\").size()[category] / len(train_input)\n",
    "    \n",
    "total_words = len(total_counter)\n",
    "\n",
    "    \n",
    "def getCondIdf(word, category): \n",
    "    #function that gets around cases where we haven't seen the word before\n",
    "    if word in group_word_idf[category]:\n",
    "        return group_word_idf[category][word]\n",
    "    else: \n",
    "        return (1 / total_words)    \n",
    "    \n",
    "def predictClassIdf(word_counter):\n",
    "    classes_prob = {}\n",
    "    for category in categories:\n",
    "        classes_prob[category] = 1\n",
    "        for k, v in word_counter.items():\n",
    "            classes_prob[category] *= (getCondIdf(k,category) ** v) * 1e5\n",
    "        classes_prob[category] *= class_priors[category]\n",
    "        #update with the prior class probability \n",
    "    return max(classes_prob, key = classes_prob.get)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(train_df, test_df, freq_cutoff): \n",
    "    ##Function Wrapper for the entire prediction process\n",
    "    \n",
    "    start = time.time()\n",
    "    total_counter = Counter(mergeSum(train_df[\"words\"]))\n",
    "    total_counter = Counter({k: v for k, v in total_counter.items() if v > freq_cutoff})\n",
    "    groups_counter = train_df.groupby(\"output\")[\"words\"].apply(lambda x: mergeSum(x))\n",
    "    groups_counter = groups_counter.apply(lambda x: Counter(x))    \n",
    "    total_wordcount = sum(total_counter.values())\n",
    "    total_word_freq = Counter({k:v/total_wordcount for k,v in total_counter.items()})\n",
    "    \n",
    "    group_word_freq = {}\n",
    "    groups_freq = groups_counter\n",
    "    for label in train_df[\"output\"].unique():\n",
    "        group_wordcount = sum(groups_counter[label].values())\n",
    "        group_word_freq[label] = Counter({k:v/group_wordcount for k,v in groups_freq[label].items()})\n",
    "\n",
    "    words_doc_list = [list(counter.keys()) for counter in list(train_df[\"words_count\"].values)]\n",
    "    words_doc_counter = Counter(chain.from_iterable(set(x) for x in words_doc_list))\n",
    "    num_docs = len(train_input)\n",
    "    total_words_idf = Counter({k: np.log(num_docs / words_doc_counter[k]) for k in total_counter.keys()})\n",
    "    \n",
    "    group_word_idf = {}\n",
    "    total_words = len(total_counter)\n",
    "    for label in train_df[\"output\"].unique():\n",
    "        temp_group = groups_counter[label]\n",
    "        group_wordcount = sum(groups_counter[label].values())\n",
    "        #Conditional probability calculation with laplace smoothing\n",
    "        group_word_idf[label] = Counter({k:((temp_group[k] + 1) / (v + total_words) * max(total_words_idf[k],0.3) )for k,v in total_counter.items()})\n",
    "\n",
    "    categories = train_df[\"output\"].unique()\n",
    "    class_priors = {}\n",
    "    for category in categories:\n",
    "        class_priors[category] = train_df.groupby(\"output\").size()[category] / len(train_input)\n",
    "    total_words = len(total_counter)\n",
    "    \n",
    "    prediction = test_df[\"words_count\"].apply(lambda x: predictClassIdf(x))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_stop = round(0.8*len(train_input))\n",
    "train_data_k = train_input[:k_stop]\n",
    "test_data_k = train_input[k_stop:]\n",
    "true_test_k = train_input[k_stop:][\"output\"]\n",
    "my_prediction_k = predict(train_data_k, test_data_k, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_cross(train_df, k, freq_cutoff): \n",
    "    #Function to do k-fold cross validation to get better accuracy statistics\n",
    "    #We partition the training set into training and test dat\n",
    "    start = time.time()\n",
    "    accuracies = []\n",
    "    \n",
    "    partition_size = round(len(train_df) / k)    \n",
    "    \n",
    "    for i in range(0,k):\n",
    "        test_data_k = train_df[(partition_size * i):(partition_size * (i + 1))]\n",
    "        train_data_k = train_df[~train_df.index.isin(test_data_k.index)]\n",
    "        my_prediction_k = predict(train_data_k, test_data_k, freq_cutoff)\n",
    "        accuracy_k = sum(my_prediction_k == test_data_k[\"output\"]) / len(test_data_k)\n",
    "        accuracies = accuracies + [accuracy_k]        \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validated_accuracies = {}\n",
    "for freq_hyper in range(0,5):\n",
    "    validated_accuracies[freq_hyper] = k_cross(train_input[0:100000],5, freq_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluate Accuracy\n",
    "raw_accuracy = sum(my_prediction_k == true_test_k) / len(true_test_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data_k[\"predicted_out\"] = my_prediction_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_input = pd.read_csv('test_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_input = generate_features(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_output_3gram1 = test_input[\"words_count\"].apply(lambda x: predictClassIdf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_output = pd.DataFrame(test_input[\"id\"])\n",
    "test_output[\"category\"] = test_output_3gram1\n",
    "test_output.to_csv(\"prediction.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
